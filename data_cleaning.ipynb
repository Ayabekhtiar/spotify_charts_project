{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline\n",
    "\n",
    "This notebook executes the data importing, merging and cleaning process using the `data_cleaning` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning.process_charts import process_all_charts\n",
    "from data_cleaning.merge import merge_data\n",
    "from data_cleaning.clean_songs import (\n",
    "    list_weekly_chart_files,\n",
    "    extract_dates_from_filenames,\n",
    "    summarize_weekly_date_gaps,\n",
    "    create_song_dict,\n",
    "    update_song_rows_with_dict,\n",
    "    fill_with_proxy_dict_compat,\n",
    "    fill_missing_from_dfs,\n",
    "    prepare_df_for_parquet,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing...\n",
      "✓ File saved as: data/silver/combined_songs.parquet\n",
      "Data processing complete.\n",
      "Starting data merging...\n",
      "✓ Merged data saved to: data/silver/songs_with_features.parquet\n",
      "Data merging complete.\n",
      "Songs loaded successfully. (41_995 rows)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "### Creating files' paths\n",
    "# Bronze files\n",
    "weekly_charts_path = os.path.join(DATA_DIR, \"bronze\", \"data\")\n",
    "tracks_path = os.path.join(DATA_DIR, \"bronze\", \"tracks.csv\")\n",
    "\n",
    "# Silver files\n",
    "songs_path = os.path.join(DATA_DIR, \"silver\", \"combined_songs.parquet\")\n",
    "output_path = os.path.join(DATA_DIR, \"silver\", \"songs_with_features.parquet\")\n",
    "\n",
    "### Merging our csvs' into parquet files in Silver for us to work with\n",
    "print(\"Starting data processing...\")\n",
    "process_all_charts(weekly_charts_path, songs_path)\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "\n",
    "print(\"Starting data merging...\")\n",
    "merge_data(tracks_path, songs_path, output_path)\n",
    "print(\"Data merging complete.\")\n",
    "\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    # read parquet\n",
    "    songs = pd.read_parquet(output_path)\n",
    "    print(\"Songs loaded successfully. ({:_} rows)\".format(songs.shape[0]))\n",
    "else:\n",
    "    raise FileNotFoundError(\"Error: Output path does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary quality check\n",
    "Firt, we want to check that our dataset spans all the desired weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date: 2016-12-29\n",
      "Last date: 2020-12-31\n",
      "Total files: 210\n",
      "Expected weeks: 210\n",
      "\n",
      "Missing weeks:\n",
      "\n",
      "Unexpected extra dates:\n"
     ]
    }
   ],
   "source": [
    "# Verification done through helper functions from data_cleaning.clean_songs\n",
    "files = list_weekly_chart_files(weekly_charts_path)\n",
    "dates = extract_dates_from_filenames(files)\n",
    "summarize_weekly_date_gaps(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to check our data quality by checking wether we are missing any values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id                0\n",
      "artist_names            0\n",
      "track_name              0\n",
      "source                  0\n",
      "streams                 0\n",
      "week_date               0\n",
      "name                10631\n",
      "popularity          10631\n",
      "duration_ms         10631\n",
      "explicit            10631\n",
      "artists             10631\n",
      "id_artists          10631\n",
      "release_date        10631\n",
      "danceability        10631\n",
      "energy              10631\n",
      "key                 10631\n",
      "loudness            10631\n",
      "mode                10631\n",
      "speechiness         10631\n",
      "acousticness        10631\n",
      "instrumentalness    10631\n",
      "liveness            10631\n",
      "valence             10631\n",
      "tempo               10631\n",
      "time_signature      10631\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are empty values in the dataset\n",
    "print(songs.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we notice that we are missing values, but it is always the same number, so we can suppose that this issue comes from the merging of our scrapped file with the kaggle file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In depth data cleaning\n",
    "### Deleting useless data\n",
    "\n",
    "First we want to see our dataset's features, such as its columns and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['track_id', 'artist_names', 'track_name', 'source', 'streams',\n",
       "        'week_date', 'name', 'popularity', 'duration_ms', 'explicit', 'artists',\n",
       "        'id_artists', 'release_date', 'danceability', 'energy', 'key',\n",
       "        'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
       "        'liveness', 'valence', 'tempo', 'time_signature'],\n",
       "       dtype='object'),\n",
       " (41995, 25))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.columns, songs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we notice that the columns \"name\" and \"track_name\" are supposed to contain the same values, but were not identified as foreign keys within the join. Therefore, we check if they both indeed contain the same information and therefore if we can remove one of them to not have duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where name is not included in track_name: 0/41995\n",
      "Column 'name' dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "def drop_column_if_exists(df, column):\n",
    "    \"\"\"\n",
    "    Drops the specified column from the DataFrame if it exists.\n",
    "    Returns the modified DataFrame.\n",
    "    \"\"\"\n",
    "    if column in df.columns:\n",
    "        print(f\"Rows where {column} is not included in track_name: {df[df[column].notna() & df['track_name'].isna()].shape[0]}/{df.shape[0]}\")\n",
    "        df = df.drop(columns=[column])\n",
    "        print(f\"Column '{column}' dropped successfully.\")\n",
    "    return df\n",
    "\n",
    "songs = drop_column_if_exists(songs, \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can recheck our columns and name to check that we haven't damaged the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['track_id', 'artist_names', 'track_name', 'source', 'streams',\n",
       "        'week_date', 'popularity', 'duration_ms', 'explicit', 'artists',\n",
       "        'id_artists', 'release_date', 'danceability', 'energy', 'key',\n",
       "        'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
       "        'liveness', 'valence', 'tempo', 'time_signature'],\n",
       "       dtype='object'),\n",
       " (41995, 24))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.columns, songs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create unique IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also notice, when looking in our dataset, that we sometimes have the same songs in the charts, but under different track_ID. That is because sometimes a singer would publish the same music multiple times (firstly as an EP or a mixtape, then in an album, then in a best of etc...). But in the case of our study, it is important that these songs have the same ID as they are the same music. \n",
    "\n",
    "Therefore we identify the songs and unify their track_IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a dictionnary of all the songs with similar artists and title and associate them with a unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 41995/41995 [00:00<00:00, 60911.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0VjIjW4GlUZAMYd2vXMi3b', 'Republic Records', Timestamp('2020-03-20 00:00:00')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of canonical song IDs using helper from data_cleaning.clean_songs\n",
    "song_dict = create_song_dict(songs)\n",
    "print(song_dict[(\"The Weeknd\", \"Blinding Lights\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we update the IDs within the dataset, so that every song has its own ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating songs: 100%|██████████| 41995/41995 [00:05<00:00, 8201.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs updated: 6_458/41_995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the update function from data_cleaning.clean_songs\n",
    "songs = update_song_rows_with_dict(songs, song_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these new unique ID, we can fill the information about song that had different IDs but were the similar music, that had been ignored by the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows filled: 11_069\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in columns of interest using helpers from data_cleaning.clean_songs\n",
    "columns_to_fill = [\n",
    "    'artist_names', 'track_name', 'source', 'duration_ms', 'explicit', \n",
    "    'popularity', 'artists', 'id_artists', 'release_date', 'danceability', 'energy', \n",
    "    'key', 'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "    'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'\n",
    "]\n",
    "\n",
    "# If we already have values for a track somewhere and in another orw it is NaN, we fill it with what we have\n",
    "songs = fill_with_proxy_dict_compat(songs, columns_to_fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 125_826 missing values in the DataFrame\n"
     ]
    }
   ],
   "source": [
    "print(\"There are still {:_} missing values in the DataFrame\".format(songs.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete our missing data through another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are still songs which are missing some data. Therefore we decide to enrich those values through other diverse datasets, found on kaggle, that also extract the official spotify audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the new dataset\n",
    "\n",
    "#Create the paths\n",
    "df_enrichment2_path = os.path.join(DATA_DIR, \"bronze\", \"spotify_top_songs_audio_features.csv\")\n",
    "df_enrichment2 = pd.read_csv(df_enrichment2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column ID will serve as a foreign key, so we rename the column ID by Track_ID, and fill the values so that the same songs have the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6513, 19),\n",
       " Index(['track_id', 'artist_names', 'track_name', 'source', 'key', 'mode',\n",
       "        'time_signature', 'danceability', 'energy', 'speechiness',\n",
       "        'acousticness', 'instrumentalness', 'liveness', 'valence', 'loudness',\n",
       "        'tempo', 'duration_ms', 'weeks_on_chart', 'streams'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correspondance between the IDs\n",
    "if \"id\" in df_enrichment2.columns:\n",
    "    # Replace id with track_id\n",
    "    df_enrichment2.rename(columns={\"id\": \"track_id\"}, inplace=True)\n",
    "df_enrichment2.shape, df_enrichment2.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((247035, 17),\n",
       " Index(['artist_name', 'track_id', 'track_name', 'acousticness', 'danceability',\n",
       "        'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness',\n",
       "        'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence',\n",
       "        'popularity'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Import and merge all the dataset within the enrichment folder. (no merging issue as they all follow the same format)\n",
    "\n",
    "# List all dataframes in a kaggle_enrichment3_dir, then add them together to it is one big dataframe\n",
    "kaggle_enrichment3_dir = os.path.join(DATA_DIR, \"bronze\", \"kaggle_enrichment3\")\n",
    "import glob\n",
    "\n",
    "# List all CSV files in the kaggle_enrichment3_dir\n",
    "csv_files = glob.glob(os.path.join(kaggle_enrichment3_dir, \"*.csv\"))\n",
    "\n",
    "# Read each CSV file into a DataFrame and collect them in a list\n",
    "df_list = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "# Concatenate all DataFrames into a single big DataFrame\n",
    "df_enrichment3 = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Show shape and columns to confirm final structure\n",
    "df_enrichment3.shape, df_enrichment3.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check that the columns we need to fill can indeed be filled by these new dataset. \n",
    "\n",
    "To do so we compare the list of columns to fill with the columns in our enrichment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'streams', 'track_id', 'weeks_on_chart'}, {'artist_name', 'track_id'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_enrichment2.columns).difference(set(columns_to_fill)), set(df_enrichment3.columns).difference(set(columns_to_fill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe that 'streams', 'weeks_on_chart' are absent from the second enrichment dataset, and that 'artist_name' is absent from the third (track_id being considered absent as the column is named ID in the dataframe, but as it will be used as a foreign key, it poses no issue for the completion).\n",
    "\n",
    "Since there is no column missing from both dataset, we can use these to fill our missing values.\n",
    "\n",
    "In terms of data quality, we know they are also official music features from spotify, so they have the same format as the data we already have within our original dataset. (And we checked in the datasets' documentations just in case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can now fill our missing values !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values *before* processing DF: 125_826\n",
      "Available columns: ['artist_names', 'track_name', 'source', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
      "Size of lookup dictionary : 6513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching songs: 100%|██████████| 41995/41995 [00:12<00:00, 3359.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['track_name', 'duration_ms', 'popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
      "Size of lookup dictionary : 130989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching songs: 100%|██████████| 41995/41995 [00:03<00:00, 11736.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values *after* processing DF: 33_889\n"
     ]
    }
   ],
   "source": [
    "# Enrich missing values from external enrichment DataFrames using helper from data_cleaning.clean_songs\n",
    "songs_gold = fill_missing_from_dfs(songs, columns_to_fill, \"track_id\", df_enrichment2, df_enrichment3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that our filling has gone smoothly and check our NA values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id               0\n",
      "artist_names           0\n",
      "track_name             0\n",
      "source                 0\n",
      "streams                0\n",
      "week_date              0\n",
      "popularity          5587\n",
      "duration_ms            0\n",
      "explicit            6966\n",
      "artists             6966\n",
      "id_artists          6966\n",
      "release_date        7404\n",
      "danceability           0\n",
      "energy                 0\n",
      "key                    0\n",
      "loudness               0\n",
      "mode                   0\n",
      "speechiness            0\n",
      "acousticness           0\n",
      "instrumentalness       0\n",
      "liveness               0\n",
      "valence                0\n",
      "tempo                  0\n",
      "time_signature         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(songs_gold.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of our NA values come from the column release date. Since it won't be really useful in our analysis, we can simply remove it. \n",
    "\n",
    "Similarly, the measure popularity seems in our case useless for the analysis since it does not relate to audio features. \n",
    "\n",
    "Therefore we can drop our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get rid of popularity and release_date as they will not be used for further analysis\n",
    "\n",
    "if \"popularity\" in songs_gold.columns:\n",
    "    songs_gold.drop(columns=[\"popularity\"], inplace=True)\n",
    "    \n",
    "if \"release_date\" in songs_gold.columns:\n",
    "    songs_gold.drop(columns=[\"release_date\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another check of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id               0\n",
      "artist_names           0\n",
      "track_name             0\n",
      "source                 0\n",
      "streams                0\n",
      "week_date              0\n",
      "duration_ms            0\n",
      "explicit            6966\n",
      "artists             6966\n",
      "id_artists          6966\n",
      "danceability           0\n",
      "energy                 0\n",
      "key                    0\n",
      "loudness               0\n",
      "mode                   0\n",
      "speechiness            0\n",
      "acousticness           0\n",
      "instrumentalness       0\n",
      "liveness               0\n",
      "valence                0\n",
      "tempo                  0\n",
      "time_signature         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the number of NaN values for each column in songs_gold\n",
    "print(songs_gold.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could do our analysis without the artists' data (although it would be less practical to identify tracks), the feature explicit is interesting to our study. \n",
    "Therefore, we want to check if the rows that are missing the explicit data are the same as the others with NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total rows with NaN values 6966\n",
      "Complete Rows / Total Rows: 35029/41995\n"
     ]
    }
   ],
   "source": [
    "songs_gold_witouht_nan = songs_gold.dropna()\n",
    "print(f\"Number of total rows with NaN values {songs_gold.shape[0] - songs_gold_witouht_nan.shape[0]}\")\n",
    "print(\"Complete Rows / Total Rows: {}/{}\".format(songs_gold_witouht_nan.shape[0], songs_gold.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of total rows with missing values is the same than the number of rows with missing values in the \"explicit\" column, it means that all missing values are within the same rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save gold dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to keep \"explicit\", we decide to keep the dataset without the rows where it is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this dataset is not missing any value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id            0\n",
      "artist_names        0\n",
      "track_name          0\n",
      "source              0\n",
      "streams             0\n",
      "week_date           0\n",
      "duration_ms         0\n",
      "explicit            0\n",
      "artists             0\n",
      "id_artists          0\n",
      "danceability        0\n",
      "energy              0\n",
      "key                 0\n",
      "loudness            0\n",
      "mode                0\n",
      "speechiness         0\n",
      "acousticness        0\n",
      "instrumentalness    0\n",
      "liveness            0\n",
      "valence             0\n",
      "tempo               0\n",
      "time_signature      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(songs_gold_witouht_nan.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No NaN ! \n",
    "\n",
    "We can therefore save our clean dataset to reuse it in our visualisation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for parquet (handles type conversions)\n",
    "songs_gold_witouht_nan_parquet = prepare_df_for_parquet(songs_gold_witouht_nan)\n",
    "\n",
    "songs_gold_witouht_nan_parquet.to_parquet(os.path.join(DATA_DIR, \"gold\", \"songs_with_features.parquet\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find here commented python lines that we wanted to use to fill in the explicit column. As it employs a private API, we decided that dropping the rows was better for this exercise. \n",
    "\n",
    "However, we wanted to keep our solution for the record, in case this analysis is used again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to use Gemini to enrich explicit values\n",
    "# from data_cleaning.artist_mapping import (\n",
    "#     get_unique_ids_from_column,\n",
    "#     get_all_combinations,\n",
    "#     get_artist_to_id,\n",
    "#     update_id_artists_with_mapping,\n",
    "# )\n",
    "# from data_cleaning.explicit_enrichment import (\n",
    "#     gemini_check_if_explicit,\n",
    "#     enrich_explicit_via_gemini,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions moved to data_cleaning.artist_mapping module\n",
    "# (get_all_combinations, get_artist_to_id, update_id_artists_with_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
