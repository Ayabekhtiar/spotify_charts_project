{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline\n",
    "\n",
    "This notebook executes the data cleaning and merging process using the `data_cleaning` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning.artist_mapping import (\n",
    "    get_unique_ids_from_column,\n",
    "    get_all_combinations,\n",
    "    get_artist_to_id,\n",
    "    update_id_artists_with_mapping,\n",
    ")\n",
    "from data_cleaning.explicit_enrichment import (\n",
    "    gemini_check_if_explicit,\n",
    "    enrich_explicit_via_gemini,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing...\n",
      "✓ File saved as: data/silver/combined_songs.parquet\n",
      "Data processing complete.\n",
      "Starting data merging...\n",
      "✓ Merged data saved to: data/silver/songs_with_features.parquet\n",
      "Data merging complete.\n",
      "Songs loaded successfully. (41_995 rows)\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning.process_charts import process_all_charts\n",
    "from data_cleaning.merge import merge_data\n",
    "from data_cleaning.clean_songs import (\n",
    "    list_weekly_chart_files,\n",
    "    extract_dates_from_filenames,\n",
    "    summarize_weekly_date_gaps,\n",
    "    create_song_dict,\n",
    "    update_song_rows_with_dict,\n",
    "    fill_with_proxy_dict_compat,\n",
    "    fill_missing_from_dfs,\n",
    "    prepare_df_for_parquet,\n",
    ")\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# files created and collected\n",
    "weekly_charts_path = os.path.join(DATA_DIR, \"bronze\", \"data\")\n",
    "tracks_path = os.path.join(DATA_DIR, \"bronze\", \"tracks.csv\")\n",
    "\n",
    "# files created by merging and cleaning\n",
    "songs_path = os.path.join(DATA_DIR, \"silver\", \"combined_songs.parquet\")\n",
    "output_path = os.path.join(DATA_DIR, \"silver\", \"songs_with_features.parquet\")\n",
    "\n",
    "print(\"Starting data processing...\")\n",
    "process_all_charts(weekly_charts_path, songs_path)\n",
    "\n",
    "print(\"Data processing complete.\")\n",
    "\n",
    "print(\"Starting data merging...\")\n",
    "merge_data(tracks_path, songs_path, output_path)\n",
    "print(\"Data merging complete.\")\n",
    "\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    # read parquet\n",
    "    songs = pd.read_parquet(output_path)\n",
    "    print(\"Songs loaded successfully. ({:_} rows)\".format(songs.shape[0]))\n",
    "else:\n",
    "    raise FileNotFoundError(\"Error: Output path does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "Check if no week was skipped during the webscraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date: 2016-12-29\n",
      "Last date: 2020-12-31\n",
      "Total files: 210\n",
      "Expected weeks: 210\n",
      "\n",
      "Missing weeks:\n",
      "\n",
      "Unexpected extra dates:\n"
     ]
    }
   ],
   "source": [
    "# Example usage of verification helpers from data_cleaning.clean_songs\n",
    "files = list_weekly_chart_files(weekly_charts_path)\n",
    "dates = extract_dates_from_filenames(files)\n",
    "summarize_weekly_date_gaps(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id                0\n",
      "artist_names            0\n",
      "track_name              0\n",
      "source                  0\n",
      "streams                 0\n",
      "week_date               0\n",
      "name                10631\n",
      "popularity          10631\n",
      "duration_ms         10631\n",
      "explicit            10631\n",
      "artists             10631\n",
      "id_artists          10631\n",
      "release_date        10631\n",
      "danceability        10631\n",
      "energy              10631\n",
      "key                 10631\n",
      "loudness            10631\n",
      "mode                10631\n",
      "speechiness         10631\n",
      "acousticness        10631\n",
      "instrumentalness    10631\n",
      "liveness            10631\n",
      "valence             10631\n",
      "tempo               10631\n",
      "time_signature      10631\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merging with kaggle introduces nan values when the track id is not in the kaggle dataframe\n",
    "print(songs.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where name is not included in track_name: 0/41_995\n",
      "Column 'name' dropped successfully.\n"
     ]
    }
   ],
   "source": [
    "if \"name\" in songs.columns:\n",
    "    # flag where name not NaN and track_name is NaN\n",
    "    print(\"Rows where name is not included in track_name: {}/{:_}\".format(songs[songs[\"name\"].notna() & songs[\"track_name\"].isna()].shape[0], songs.shape[0]))\n",
    "    \n",
    "    # Drop the \"name\" column as it is included in \"track_name\"\n",
    "    songs.drop(columns=[\"name\"], inplace=True)\n",
    "    print(\"Column 'name' dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['track_id', 'artist_names', 'track_name', 'source', 'streams',\n",
       "        'week_date', 'popularity', 'duration_ms', 'explicit', 'artists',\n",
       "        'id_artists', 'release_date', 'danceability', 'energy', 'key',\n",
       "        'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
       "        'liveness', 'valence', 'tempo', 'time_signature'],\n",
       "       dtype='object'),\n",
       " (41995, 24))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.columns, songs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 41995/41995 [00:00<00:00, 56948.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0VjIjW4GlUZAMYd2vXMi3b', 'Republic Records', Timestamp('2020-03-20 00:00:00')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of canonical song IDs using helper from data_cleaning.clean_songs\n",
    "song_dict = create_song_dict(songs)\n",
    "print(song_dict[(\"The Weeknd\", \"Blinding Lights\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating songs: 100%|██████████| 41995/41995 [00:08<00:00, 4871.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs updated: 6_458/41_995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the update function from data_cleaning.clean_songs\n",
    "songs = update_song_rows_with_dict(songs, song_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows filled: 11_069\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in columns of interest using helpers from data_cleaning.clean_songs\n",
    "columns_to_fill = [\n",
    "    'artist_names', 'track_name', 'source', 'duration_ms', 'explicit', \n",
    "    'popularity', 'artists', 'id_artists', 'release_date', 'danceability', 'energy', \n",
    "    'key', 'loudness', 'mode', 'speechiness', 'acousticness', \n",
    "    'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'\n",
    "]\n",
    "\n",
    "# If we already have values for a track somewhere and in another orw it is NaN, we fill it with what we have\n",
    "songs = fill_with_proxy_dict_compat(songs, columns_to_fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 7_404 rows with NaN values\n"
     ]
    }
   ],
   "source": [
    "print(\"There are still {:_} rows with NaN values\".format(songs[songs.isna().any(axis=1)].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich still missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6513, 19),\n",
       " Index(['track_id', 'artist_names', 'track_name', 'source', 'key', 'mode',\n",
       "        'time_signature', 'danceability', 'energy', 'speechiness',\n",
       "        'acousticness', 'instrumentalness', 'liveness', 'valence', 'loudness',\n",
       "        'tempo', 'duration_ms', 'weeks_on_chart', 'streams'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enrichment2_path = os.path.join(DATA_DIR, \"bronze\", \"spotify_top_songs_audio_features.csv\")\n",
    "df_enrichment2 = pd.read_csv(df_enrichment2_path)\n",
    "if \"id\" in df_enrichment2.columns:\n",
    "    # Replace id with track_id\n",
    "    df_enrichment2.rename(columns={\"id\": \"track_id\"}, inplace=True)\n",
    "df_enrichment2.shape, df_enrichment2.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((247035, 17),\n",
       " Index(['artist_name', 'track_id', 'track_name', 'acousticness', 'danceability',\n",
       "        'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness',\n",
       "        'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence',\n",
       "        'popularity'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all dataframes in a kaggle_enrichment3_dir, then add them together to it is one big dataframe\n",
    "kaggle_enrichment3_dir = os.path.join(DATA_DIR, \"bronze\", \"kaggle_enrichment3\")\n",
    "import glob\n",
    "\n",
    "# List all CSV files in the kaggle_enrichment3_dir\n",
    "csv_files = glob.glob(os.path.join(kaggle_enrichment3_dir, \"*.csv\"))\n",
    "\n",
    "# Read each CSV file into a DataFrame and collect them in a list\n",
    "df_list = [pd.read_csv(f) for f in csv_files]\n",
    "\n",
    "# Concatenate all DataFrames into a single big DataFrame\n",
    "df_enrichment3 = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Show shape and columns to confirm final structure\n",
    "df_enrichment3.shape, df_enrichment3.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'streams', 'track_id', 'weeks_on_chart'}, {'artist_name', 'track_id'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_enrichment2.columns).difference(set(columns_to_fill)), set(df_enrichment3.columns).difference(set(columns_to_fill))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values *before* processing DF: 125_826\n",
      "Available columns: ['artist_names', 'track_name', 'source', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
      "Size of lookup dictionary : 6513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching songs: 100%|██████████| 41995/41995 [00:13<00:00, 3199.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['track_name', 'duration_ms', 'popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']\n",
      "Size of lookup dictionary : 130989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching songs: 100%|██████████| 41995/41995 [00:03<00:00, 12373.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values *after* processing DF: 33_889\n"
     ]
    }
   ],
   "source": [
    "# Enrich missing values from external enrichment DataFrames using helper from data_cleaning.clean_songs\n",
    "songs_gold = fill_missing_from_dfs(songs, columns_to_fill, \"track_id\", df_enrichment2, df_enrichment3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save songs_gold\n",
    "if \"popularity\" in songs_gold.columns:\n",
    "    songs_gold.drop(columns=[\"popularity\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id               0\n",
      "artist_names           0\n",
      "track_name             0\n",
      "source                 0\n",
      "streams                0\n",
      "week_date              0\n",
      "duration_ms            0\n",
      "explicit            6966\n",
      "artists             6966\n",
      "id_artists          6966\n",
      "release_date        7404\n",
      "danceability           0\n",
      "energy                 0\n",
      "key                    0\n",
      "loudness               0\n",
      "mode                   0\n",
      "speechiness            0\n",
      "acousticness           0\n",
      "instrumentalness       0\n",
      "liveness               0\n",
      "valence                0\n",
      "tempo                  0\n",
      "time_signature         0\n",
      "dtype: int64\n",
      "7404\n",
      "34591/41995\n"
     ]
    }
   ],
   "source": [
    "# Display the number of NaN values for each column in songs_gold\n",
    "songs_gold_witouht_nan = songs_gold.dropna()\n",
    "\n",
    "print(songs_gold.isna().sum())\n",
    "print(songs_gold.shape[0] - songs_gold_witouht_nan.shape[0])\n",
    "print(\"{}/{}\".format(songs_gold_witouht_nan.shape[0], songs_gold.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>week_date</th>\n",
       "      <th>explicit</th>\n",
       "      <th>artists</th>\n",
       "      <th>id_artists</th>\n",
       "      <th>release_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25sgk305KZfyuqVBQIahim</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7wFybC8jBH3zE139OpCtpG</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0jAfdqv18goRTUxm3ilRjb</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>13hvHEstJ4sNbzdroPrPI3</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Dua Lipa', 'BLACKPINK']</td>\n",
       "      <td>['6M2wZ9GZgrQXHCFfjv46we', '41MozSoPIsD1dJM0CL...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2FUNBaa5DwItJtYEBgAblU</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41943</th>\n",
       "      <td>7K7MUBCnzgBAvMVW2RTWNs</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41951</th>\n",
       "      <td>6fxVffaTuwjgEk5h9QyRjy</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Ed Sheeran']</td>\n",
       "      <td>['6eUKZXaKkcviH0Ku9w2n3V']</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41978</th>\n",
       "      <td>5s8LepdwU0THzpd0M7nLsa</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41986</th>\n",
       "      <td>4Sokm1cWK36H2WctWWRGf1</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41994</th>\n",
       "      <td>5AiqL0ezAAauDuVK5EJvXU</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7404 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     track_id   week_date  explicit  \\\n",
       "5      25sgk305KZfyuqVBQIahim  2019-01-17       NaN   \n",
       "11     7wFybC8jBH3zE139OpCtpG  2019-01-17       NaN   \n",
       "56     0jAfdqv18goRTUxm3ilRjb  2019-01-17       NaN   \n",
       "57     13hvHEstJ4sNbzdroPrPI3  2019-01-17       0.0   \n",
       "62     2FUNBaa5DwItJtYEBgAblU  2019-01-17       NaN   \n",
       "...                       ...         ...       ...   \n",
       "41943  7K7MUBCnzgBAvMVW2RTWNs  2019-06-13       NaN   \n",
       "41951  6fxVffaTuwjgEk5h9QyRjy  2019-06-13       0.0   \n",
       "41978  5s8LepdwU0THzpd0M7nLsa  2019-06-13       NaN   \n",
       "41986  4Sokm1cWK36H2WctWWRGf1  2019-06-13       NaN   \n",
       "41994  5AiqL0ezAAauDuVK5EJvXU  2019-06-13       NaN   \n",
       "\n",
       "                         artists  \\\n",
       "5                           None   \n",
       "11                          None   \n",
       "56                          None   \n",
       "57     ['Dua Lipa', 'BLACKPINK']   \n",
       "62                          None   \n",
       "...                          ...   \n",
       "41943                       None   \n",
       "41951             ['Ed Sheeran']   \n",
       "41978                       None   \n",
       "41986                       None   \n",
       "41994                       None   \n",
       "\n",
       "                                              id_artists release_date  \n",
       "5                                                   None         None  \n",
       "11                                                  None         None  \n",
       "56                                                  None         None  \n",
       "57     ['6M2wZ9GZgrQXHCFfjv46we', '41MozSoPIsD1dJM0CL...         None  \n",
       "62                                                  None         None  \n",
       "...                                                  ...          ...  \n",
       "41943                                               None         None  \n",
       "41951                         ['6eUKZXaKkcviH0Ku9w2n3V']         None  \n",
       "41978                                               None         None  \n",
       "41986                                               None         None  \n",
       "41994                                               None         None  \n",
       "\n",
       "[7404 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rows with NaN values in any column for songs_gold\n",
    "songs_gold_with_nan = songs_gold[songs_gold.isna().any(axis=1)][[\"track_id\", \"week_date\", \"explicit\", \"artists\", \"id_artists\", \"release_date\"]]\n",
    "songs_gold_with_nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decide to drop the rows without release date as they are the same rows as teh ones without explicit, artists, id_artists and release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id            0\n",
      "artist_names        0\n",
      "track_name          0\n",
      "source              0\n",
      "streams             0\n",
      "week_date           0\n",
      "duration_ms         0\n",
      "explicit            0\n",
      "artists             0\n",
      "id_artists          0\n",
      "release_date        0\n",
      "danceability        0\n",
      "energy              0\n",
      "key                 0\n",
      "loudness            0\n",
      "mode                0\n",
      "speechiness         0\n",
      "acousticness        0\n",
      "instrumentalness    0\n",
      "liveness            0\n",
      "valence             0\n",
      "tempo               0\n",
      "time_signature      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(songs_gold_witouht_nan.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataframe for parquet (handles type conversions)\n",
    "songs_gold_witouht_nan_parquet = prepare_df_for_parquet(songs_gold_witouht_nan)\n",
    "\n",
    "songs_gold_witouht_nan_parquet.to_parquet(os.path.join(DATA_DIR, \"gold\", \"songs_with_features.parquet\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions moved to data_cleaning.artist_mapping module\n",
    "# (get_all_combinations, get_artist_to_id, update_id_artists_with_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
